[setup]
# random_seed: seed for random number generator to have reproducible results
# total_epochs: total number of training epochs
# anchor_epoch: number of epochs until adding anchors
random_seed = 1
total_epochs = 1000
anchor_epoch = 500

# plot_freq: frequency for generating and saving plots
# print_freq: frequency for printing statistics
# save_freq: frequency for saving data for csv file
plot_freq = 50
print_freq = 1
save_freq = 1

# checkpoint_epochs: list of epochs to save checkpoints at, 0 if no checkpoints to be saved
# prev_checkpoint: (optional) filepath of checkpoint to load from 
checkpoint_epochs = 0
prev_checkpoint =

# label to be saved with results
label = BASE

[dataset]
# plaut: filepath for plaut dataset file
# anchor: filepath for anchor dataset file
# probe: filepath for probe dataset file

# List of useful filepaths:
# original plaut: ../dataset/plaut_dataset.csv
# collapsed plaut: ../dataset/plaut_dataset_collapsed.csv
# old anchors and probes: ../dataset/anchors.csv, ../dataset/probes.csv
# new anchors: (N) ../dataset/anchors_new1.csv, (N/2)../dataset/anchors_new2.csv, (N/3)../dataset/anchors_new3.csv
# new probes: ../dataset/probes_new.csv


plaut = ../dataset/plaut_dataset_collapsed.csv
anchor = ../dataset/anchors_swap1.csv
probe = ../dataset/probes_new.csv

# DEFINITION OF DIFFERENT OPTIMIZER SETTINGS
#  > start: epoch to switch to new optimizer
#  > optim: optimizer (ONLY Adam or SGD)
#  > lr: learning rate (0.001 used in paper)
#  > momentum: momentum (0 initially, then 0.9 used in paper)
#    NOTE: a momentum value MUST be included (it will simply be ignored for Adam)
#  > wd: weight decay (0.0001 used in paper)
# NOTE: use a new [partX] for each time the optimizer needs to be changed

[part1]
start = 0
optim = SGD
lr = 0.001
momentum = 0
wd = 0.000001

[part2]
start = 10
optim = Adam
lr = 0.01
momentum = 0
wd = 0.000001

[part3]
start = 500
optim = Adam
lr = 0.01
momentum = 0
wd = 0.000001


